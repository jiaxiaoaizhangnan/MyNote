<attachment contenteditable="false" data-atts="%5B%5D" data-aid=".atts-e55a0fbe-dc12-4314-9409-67954dc045ca"></attachment><p>tensorflow 各种层</p><p>"""</p><p>Implementing Different Layers</p><p><br></p><p>We will illustrate how to use different types of layers in TensorFlow</p><p><br></p><p>The layers of interest are:</p><p>&nbsp;(1) Convolutional Layer</p><p>&nbsp;(2) Activation Layer</p><p>&nbsp;(3) Max-Pool Layer</p><p>&nbsp;(4) Fully Connected Layer</p><p><br></p><p>We will generate two different data sets for this</p><p>&nbsp;script, a 1-D data set (row of data) and</p><p>&nbsp;a 2-D data set (similar to picture)</p><p>"""</p><p><br></p><p>import tensorflow as tf</p><p>import numpy as np</p><p>from tensorflow.python.framework import ops</p><p>ops.reset_default_graph()</p><p><br></p><p># ---------------------------------------------------|</p><p># -------------------1D-data-------------------------|</p><p># ---------------------------------------------------|</p><p><br></p><p># Create graph session</p><p>sess = tf.Session()</p><p><br></p><p># parameters for the run</p><p>data_size = 25</p><p>conv_size = 5</p><p>maxpool_size = 5</p><p>stride_size = 1</p><p><br></p><p># ensure reproducibility</p><p>seed = 13</p><p>np.random.seed(seed)</p><p>tf.set_random_seed(seed)</p><p><br></p><p># Generate 1D data</p><p>data_1d = np.random.normal(size=data_size)</p><p><br></p><p># Placeholder</p><p>x_input_1d = tf.placeholder(dtype=tf.float32, shape=[data_size])</p><p><br></p><p><br></p><p># --------Convolution--------</p><p>def conv_layer_1d(input_1d, input_filter, stride):</p><p>&nbsp;&nbsp;"""</p><p>&nbsp;&nbsp;TensorFlow's 'conv2d()' function only works with 4D arrays:</p><p>&nbsp;&nbsp;[batch#, width, height, channels], we have 1 batch, and</p><p>&nbsp;&nbsp;width = 1, but height = the length of the input, and 1 channel.</p><p>&nbsp;&nbsp;So next we create the 4D array by inserting dimension 1's.</p><p>&nbsp;&nbsp;:param input_1d: 1D input array.</p><p>&nbsp;&nbsp;:param input_filter: Filter to convolve across the input_1d array.</p><p>&nbsp;&nbsp;:param stride: stride for filter.</p><p>&nbsp;&nbsp;:return: array.</p><p>&nbsp;&nbsp;"""</p><p>&nbsp;&nbsp;input_2d = tf.expand_dims(input_1d, 0)</p><p>&nbsp;&nbsp;input_3d = tf.expand_dims(input_2d, 0)</p><p>&nbsp;&nbsp;input_4d = tf.expand_dims(input_3d, 3)</p><p>&nbsp;&nbsp;# Perform convolution with stride = 1, if we wanted to increase the stride,</p><p>&nbsp;&nbsp;# to say '2', then strides=[1,1,2,1]</p><p>&nbsp;&nbsp;convolution_output = tf.nn.conv2d(input_4d,</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;filter=input_filter,</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;strides=[1, 1, stride, 1],</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;padding="VALID")</p><p>&nbsp;&nbsp;# Get rid of extra dimensions</p><p>&nbsp;&nbsp;conv_output_1d = tf.squeeze(convolution_output)</p><p>&nbsp;&nbsp;return conv_output_1d</p><p><br></p><p># Create filter for convolution.</p><p>my_filter = tf.Variable(tf.random_normal(shape=[1, conv_size, 1, 1]))</p><p># Create convolution layer</p><p>my_convolution_output = conv_layer_1d(x_input_1d, my_filter, stride=stride_size)</p><p><br></p><p><br></p><p># --------Activation--------</p><p>def activation(input_1d):</p><p>&nbsp;&nbsp;return tf.nn.relu(input_1d)</p><p><br></p><p># Create activation layer</p><p>my_activation_output = activation(my_convolution_output)</p><p><br></p><p><br></p><p># --------Max Pool--------</p><p>def max_pool(input_1d, width, stride):</p><p>&nbsp;&nbsp;"""</p><p>&nbsp;&nbsp;Just like 'conv2d()' above, max_pool() works with 4D arrays.</p><p>&nbsp;&nbsp;[batch_size=1, width=1, height=num_input, channels=1]</p><p>&nbsp;&nbsp;:param input_1d: Input array to perform max-pool on.</p><p>&nbsp;&nbsp;:param width: Width of 1d-window for max-pool</p><p>&nbsp;&nbsp;:param stride: Stride of window across input array</p><p>&nbsp;&nbsp;:return: max-pooled array</p><p>&nbsp;&nbsp;"""</p><p>&nbsp;&nbsp;input_2d = tf.expand_dims(input_1d, 0)</p><p>&nbsp;&nbsp;input_3d = tf.expand_dims(input_2d, 0)</p><p>&nbsp;&nbsp;input_4d = tf.expand_dims(input_3d, 3)</p><p>&nbsp;&nbsp;# Perform the max pooling with strides = [1,1,1,1]</p><p>&nbsp;&nbsp;# If we wanted to increase the stride on our data dimension, say by</p><p>&nbsp;&nbsp;# a factor of '2', we put strides = [1, 1, 2, 1]</p><p>&nbsp;&nbsp;# We will also need to specify the width of the max-window ('width')</p><p>&nbsp;&nbsp;pool_output = tf.nn.max_pool(input_4d, ksize=[1, 1, width, 1],</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;strides=[1, 1, stride, 1],</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;padding='VALID')</p><p>&nbsp;&nbsp;# Get rid of extra dimensions</p><p>&nbsp;&nbsp;pool_output_1d = tf.squeeze(pool_output)</p><p>&nbsp;&nbsp;return pool_output_1d</p><p><br></p><p>my_maxpool_output = max_pool(my_activation_output, width=maxpool_size, stride=stride_size)</p><p><br></p><p><br></p><p># --------Fully Connected--------</p><p>def fully_connected(input_layer, num_outputs):</p><p>&nbsp;&nbsp;# First we find the needed shape of the multiplication weight matrix:</p><p>&nbsp;&nbsp;# The dimension will be (length of input) by (num_outputs)</p><p>&nbsp;&nbsp;weight_shape = tf.squeeze(tf.stack([tf.shape(input_layer), [num_outputs]]))</p><p>&nbsp;&nbsp;# Initialize such weight</p><p>&nbsp;&nbsp;weight = tf.random_normal(weight_shape, stddev=0.1)</p><p>&nbsp;&nbsp;# Initialize the bias</p><p>&nbsp;&nbsp;bias = tf.random_normal(shape=[num_outputs])</p><p>&nbsp;&nbsp;# Make the 1D input array into a 2D array for matrix multiplication</p><p>&nbsp;&nbsp;input_layer_2d = tf.expand_dims(input_layer, 0)</p><p>&nbsp;&nbsp;# Perform the matrix multiplication and add the bias</p><p>&nbsp;&nbsp;full_output = tf.add(tf.matmul(input_layer_2d, weight), bias)</p><p>&nbsp;&nbsp;# Get rid of extra dimensions</p><p>&nbsp;&nbsp;full_output_1d = tf.squeeze(full_output)</p><p>&nbsp;&nbsp;return full_output_1d</p><p><br></p><p>my_full_output = fully_connected(my_maxpool_output, 5)</p><p><br></p><p># Initialize Variables</p><p>init = tf.global_variables_initializer()</p><p>sess.run(init)</p><p><br></p><p>feed_dict = {x_input_1d: data_1d}</p><p><br></p><p>print('&gt;&gt;&gt;&gt; 1D Data &lt;&lt;&lt;&lt;')</p><p><br></p><p># Convolution Output</p><p>print('Input = array of length {}'.format(x_input_1d.shape.as_list()[0]))</p><p>print('Convolution w/ filter, length = {}, stride size = {},'</p><p>&nbsp;&nbsp;&nbsp;'results in an array of length {}:'.format(conv_size,</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;stride_size,</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;my_convolution_output.shape.as_list()[0]))</p><p>print(sess.run(my_convolution_output, feed_dict=feed_dict))</p><p><br></p><p># Activation Output</p><p>print('\nInput = above array of length {}'.format(my_convolution_output.shape.as_list()[0]))</p><p>print('ReLU element wise returns '</p><p>&nbsp;&nbsp;&nbsp;'an array of length {}:'.format(my_activation_output.shape.as_list()[0]))</p><p>print(sess.run(my_activation_output, feed_dict=feed_dict))</p><p><br></p><p># Max Pool Output</p><p>print('\nInput = above array of length {}'.format(my_activation_output.shape.as_list()[0]))</p><p>print('MaxPool, window length = {}, stride size = {},'</p><p>&nbsp;&nbsp;&nbsp;'results in the array of length {}'.format(maxpool_size,</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;stride_size,</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;my_maxpool_output.shape.as_list()[0]))</p><p>print(sess.run(my_maxpool_output, feed_dict=feed_dict))</p><p><br></p><p># Fully Connected Output</p><p>print('\nInput = above array of length {}'.format(my_maxpool_output.shape.as_list()[0]))</p><p>print('Fully connected layer on all 4 rows '</p><p>&nbsp;&nbsp;&nbsp;'with {} outputs:'.format(my_full_output.shape.as_list()[0]))</p><p>print(sess.run(my_full_output, feed_dict=feed_dict))</p><p><br></p><p># ---------------------------------------------------|</p><p># -------------------2D-data-------------------------|</p><p># ---------------------------------------------------|</p><p><br></p><p># Reset Graph</p><p>ops.reset_default_graph()</p><p>sess = tf.Session()</p><p><br></p><p># Parameters for the run</p><p>row_size = 10</p><p>col_size = 10</p><p>conv_size = 2</p><p>conv_stride_size = 2</p><p>maxpool_size = 2</p><p>maxpool_stride_size = 1</p><p><br></p><p># Set seed to ensure reproducibility</p><p>seed = 13</p><p>np.random.seed(seed)</p><p>tf.set_random_seed(seed)</p><p><br></p><p># Generate 2D data</p><p>data_size = [row_size, col_size]</p><p>data_2d = np.random.normal(size=data_size)</p><p><br></p><p># --------Placeholder--------</p><p>x_input_2d = tf.placeholder(dtype=tf.float32, shape=data_size)</p><p><br></p><p><br></p><p># Convolution</p><p>def conv_layer_2d(input_2d, conv_filter, conv_stride):</p><p>&nbsp;&nbsp;"""</p><p>&nbsp;&nbsp;TensorFlow's 'conv2d()' function only works with 4D arrays:</p><p>&nbsp;&nbsp;[batch#, width, height, channels], we have 1 batch, and</p><p>&nbsp;&nbsp;1 channel, but we do have width AND height this time.</p><p>&nbsp;&nbsp;So next we create the 4D array by inserting dimension 1's.</p><p>&nbsp;&nbsp;:param input_2d: input array for 2D convolution.</p><p>&nbsp;&nbsp;:param conv_filter: 2D-filter.</p><p>&nbsp;&nbsp;:param conv_stride: 2D stride settings.</p><p>&nbsp;&nbsp;:return: Convoluted array.</p><p>&nbsp;&nbsp;"""</p><p>&nbsp;&nbsp;input_3d = tf.expand_dims(input_2d, 0)</p><p>&nbsp;&nbsp;input_4d = tf.expand_dims(input_3d, 3)</p><p>&nbsp;&nbsp;# Note the stride difference below!</p><p>&nbsp;&nbsp;convolution_output = tf.nn.conv2d(input_4d,</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;filter=conv_filter,</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;strides=[1, conv_stride, conv_stride, 1],</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;padding="VALID")</p><p>&nbsp;&nbsp;# Get rid of unnecessary dimensions</p><p>&nbsp;&nbsp;conv_output_2d = tf.squeeze(convolution_output)</p><p>&nbsp;&nbsp;return conv_output_2d</p><p><br></p><p># Create Convolutional Filter</p><p>my_filter = tf.Variable(tf.random_normal(shape=[conv_size, conv_size, 1, 1]))</p><p># Create Convolutional Layer</p><p>my_convolution_output = conv_layer_2d(x_input_2d, my_filter, conv_stride=conv_stride_size)</p><p><br></p><p><br></p><p># --------Activation--------</p><p>def activation(input_1d):</p><p>&nbsp;&nbsp;return tf.nn.relu(input_1d)</p><p><br></p><p># Create Activation Layer</p><p>my_activation_output = activation(my_convolution_output)</p><p><br></p><p><br></p><p># --------Max Pool--------</p><p>def max_pool(input_2d, width, height, stride):</p><p>&nbsp;&nbsp;"""</p><p>&nbsp;&nbsp;Just like 'conv2d()' above, max_pool() works with 4D arrays.</p><p>&nbsp;&nbsp;[batch_size=1, width=given, height=given, channels=1]</p><p>&nbsp;&nbsp;:param input_2d: 2D input array</p><p>&nbsp;&nbsp;:param width: width of 2D max pool window</p><p>&nbsp;&nbsp;:param height: height of 2D max pool window</p><p>&nbsp;&nbsp;:param stride: 2d stride setting</p><p>&nbsp;&nbsp;:return: max-pool'ed array</p><p>&nbsp;&nbsp;"""</p><p>&nbsp;&nbsp;input_3d = tf.expand_dims(input_2d, 0)</p><p>&nbsp;&nbsp;input_4d = tf.expand_dims(input_3d, 3)</p><p>&nbsp;&nbsp;# Perform the max pooling with strides = [1,1,1,1]</p><p>&nbsp;&nbsp;# If we wanted to increase the stride on our data dimension, say by</p><p>&nbsp;&nbsp;# a factor of '2', we put strides = [1, 2, 2, 1]</p><p>&nbsp;&nbsp;pool_output = tf.nn.max_pool(input_4d, ksize=[1, height, width, 1],</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;strides=[1, stride, stride, 1],</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;padding='VALID')</p><p>&nbsp;&nbsp;# Get rid of unnecessary dimensions</p><p>&nbsp;&nbsp;pool_output_2d = tf.squeeze(pool_output)</p><p>&nbsp;&nbsp;return pool_output_2d</p><p><br></p><p># Create Max-Pool Layer</p><p>my_maxpool_output = max_pool(my_activation_output,&nbsp;</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;width=maxpool_size,</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;height=maxpool_size,</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;stride=maxpool_stride_size)</p><p><br></p><p><br></p><p># -------Fully Connected--------</p><p>def fully_connected(input_layer, num_outputs):</p><p>&nbsp;&nbsp;"""</p><p>&nbsp;&nbsp;In order to connect our whole W byH 2d array, we first flatten it out to</p><p>&nbsp;&nbsp;a W times H 1D array.</p><p>&nbsp;&nbsp;:param input_layer: input array for fully connected layer.</p><p>&nbsp;&nbsp;:param num_outputs: how many outputs to give from layer.</p><p>&nbsp;&nbsp;:return: array of size num_outputs</p><p>&nbsp;&nbsp;"""</p><p>&nbsp;&nbsp;flat_input = tf.reshape(input_layer, [-1])</p><p>&nbsp;&nbsp;# We then find out how long it is, and create an array for the shape of</p><p>&nbsp;&nbsp;# the multiplication weight = (WxH) by (num_outputs)</p><p>&nbsp;&nbsp;weight_shape = tf.squeeze(tf.stack([tf.shape(flat_input), [num_outputs]]))</p><p>&nbsp;&nbsp;# Initialize the weight</p><p>&nbsp;&nbsp;weight = tf.random_normal(weight_shape, stddev=0.1)</p><p>&nbsp;&nbsp;# Initialize the bias</p><p>&nbsp;&nbsp;bias = tf.random_normal(shape=[num_outputs])</p><p>&nbsp;&nbsp;# Now make the flat 1D array into a 2D array for multiplication</p><p>&nbsp;&nbsp;input_2d = tf.expand_dims(flat_input, 0)</p><p>&nbsp;&nbsp;# Multiply and add the bias</p><p>&nbsp;&nbsp;full_output = tf.add(tf.matmul(input_2d, weight), bias)</p><p>&nbsp;&nbsp;# Get rid of extra dimension</p><p>&nbsp;&nbsp;full_output_2d = tf.squeeze(full_output)</p><p>&nbsp;&nbsp;return full_output_2d</p><p><br></p><p># Create Fully Connected Layer</p><p>my_full_output = fully_connected(my_maxpool_output, 5)</p><p><br></p><p># Run graph</p><p># Initialize Variables</p><p>init = tf.global_variables_initializer()</p><p>sess.run(init)</p><p><br></p><p>feed_dict = {x_input_2d: data_2d}</p><p><br></p><p>print('\n&gt;&gt;&gt;&gt; 2D Data &lt;&lt;&lt;&lt;')</p><p><br></p><p># Convolution Output</p><p>print('Input = {} array'.format(x_input_2d.shape.as_list()))</p><p>print('{} Convolution, stride size = [{}, {}], '</p><p>&nbsp;&nbsp;&nbsp;'results in the {} array'.format(my_filter.get_shape().as_list()[:2],</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;conv_stride_size,</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;conv_stride_size,</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;my_convolution_output.shape.as_list()))</p><p>print(sess.run(my_convolution_output, feed_dict=feed_dict))</p><p><br></p><p># Activation Output</p><p>print('\nInput = the above {} array'.format(my_convolution_output.shape.as_list()))</p><p>print('ReLU element wise returns the {} array'.format(my_activation_output.shape.as_list()))</p><p>print(sess.run(my_activation_output, feed_dict=feed_dict))</p><p><br></p><p># Max Pool Output</p><p>print('\nInput = the above {} array'.format(my_activation_output.shape.as_list()))</p><p>print('MaxPool, stride size = [{}, {}], '</p><p>&nbsp;&nbsp;&nbsp;'results in {} array'.format(maxpool_stride_size,</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;maxpool_stride_size,</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;my_maxpool_output.shape.as_list()))</p><p>print(sess.run(my_maxpool_output, feed_dict=feed_dict))</p><p><br></p><p># Fully Connected Output</p><p>print('\nInput = the above {} array'.format(my_maxpool_output.shape.as_list()))</p><p>print('Fully connected layer on all {} rows '</p><p>&nbsp;&nbsp;&nbsp;'results in {} outputs:'.format(my_maxpool_output.shape.as_list()[0],</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;my_full_output.shape.as_list()[0]))</p><p>print(sess.run(my_full_output, feed_dict=feed_dict))</p><p><br></p>