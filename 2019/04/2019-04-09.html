<attachment contenteditable="false" data-atts="%5B%5D" data-aid=".atts-0cbe78bd-1b13-468c-ad4b-608d34bdf83b"></attachment><p>tensorflow 迁移学习：<a href="https://codelabs.developers.google.com/codelabs/tensorflow-for-poets/#0" target="_blank">https://codelabs.developers.google.com/codelabs/tensorflow-for-poets/#0</a></p><p><br></p><h1>What Does A Face Detection Neural Network Look&nbsp;Like?</h1><p><a href="https://towardsdatascience.com/@reina.wang?source=post_header_lockup" target="_blank" style="color: inherit; background-color: transparent;"><img src="https://cdn-images-1.medium.com/fit/c/50/50/1*diRpAT2jWilemimLhfwHzw.jpeg" alt="Go to the profile of Chi-Feng Wang"></a></p><p><a href="https://towardsdatascience.com/@reina.wang" target="_blank" style="color: rgba(0, 0, 0, 0.84); background-color: transparent;">Chi-Feng Wang</a></p><p>Jul 24, 2018</p><p>In my&nbsp;<a href="https://medium.com/@reina.wang/mtcnn-face-detection-cdcb20448ce0" target="_blank" style="color: inherit; background-color: transparent;">last post</a>, I explored the Multi-task Cascaded Convolutional Network (MTCNN) model, using it to detect faces with my webcam. In this post, I will examine the structure of the neural network.</p><p>The MTCNN model consists of 3 separate networks: the P-Net, the R-Net, and the O-Net:</p><p><img src="https://cdn-images-1.medium.com/max/800/1*ICM3jnRB1unY6G5ZRGorfg.png"></p><p>Image 1: MTCNN Structure //&nbsp;<a href="https://arxiv.org/ftp/arxiv/papers/1604/1604.02878.pdf" target="_blank" style="color: inherit; background-color: transparent;">Source</a></p><p>For every image we pass in, the network creates an image pyramid: that is, it creates multiple copies of that image in different sizes.</p><p><img src="https://cdn-images-1.medium.com/max/800/1*JH-L5EmTqj_fHEcXnzZT5Q.png"></p><p>Image 2: Image Pyramid //&nbsp;<a href="https://arxiv.org/ftp/arxiv/papers/1604/1604.02878.pdf" target="_blank" style="color: inherit; background-color: transparent;">Source</a></p><p>In the P-Net, for each scaled image, a 12x12 kernel runs through the image, searching for a face. In the image below, the red square represents the kernel, which slowly moves across and down the image, searching for a face.</p><p><img src="https://cdn-images-1.medium.com/max/800/1*r3aAaOV2CWYBJav3uWRP5A.png"></p><p>Image 3: 12x12 kernel in the top right corner. After scanning this corner, it shifts sideways (or downwards) by 1 pixel, and continues doing that until it has gone through the entire&nbsp;image.</p><p>Within each of these 12x12 kernels, 3 convolutions are run through (If you don’t know what convolutions are, check out&nbsp;<a href="https://medium.com/@reina.wang.tw/what-is-a-neural-network-6010edabde2b" target="_blank" style="color: inherit; background-color: transparent;">my other article</a>&nbsp;or&nbsp;<a href="http://setosa.io/ev/image-kernels/" target="_blank" style="color: inherit; background-color: transparent;">this site</a>) with 3x3 kernels. After every convolution layer, a prelu layer is implemented (when you multiply every negative pixel with a certain number ‘alpha’. ‘Alpha’ is to be determined through training). In addition, a maxpool layer is put in after the first prelu layer(maxpool takes out every other pixel, leaving only the largest one in the vicinity).</p><p><img src="https://cdn-images-1.medium.com/max/800/1*w49KKbft4Iq3xLsoNy-l-Q.png"></p><p>Image 4: Max-pool //&nbsp;<a href="https://youtu.be/gbceqO8PpBg" target="_blank" style="color: inherit; background-color: transparent;">Source</a></p><p>After the third convolution layer, the network splits into two layers. The activations from the third layer are passed to two separate convolution layers, and a softmax layer after one of those convolution layers (softmax assigns decimal probabilities to every result, and the probabilities add up to 1. In this case, it outputs 2 probabilities: the probability that there&nbsp;<strong>is</strong>&nbsp;a face in the area and the probability that there&nbsp;<strong>isn’t&nbsp;</strong>a face).</p><p><img src="https://cdn-images-1.medium.com/max/800/1*Ey4E1ZreYY8F_GiXLVCD_Q.png"></p><p>Image 5:&nbsp;P-Net</p><p>Convolution 4–1 outputs the probability of a face being in each bounding box, and convolution 4–2 outputs the coordinates of the bounding boxes.</p><p>Taking a look at mtcnn.py will show you the structure of P-Net:</p><pre class="ql-syntax" spellcheck="false">class PNet(Network):
def _config(self):
layer_factory = LayerFactory(self)
layer_factory.new_feed(name='data', layer_shape=(None, None, None, 3))
layer_factory.new_conv(name='conv1', kernel_size=(3, 3), channels_output=10, stride_size=(1, 1),
padding='VALID', relu=False)
layer_factory.new_prelu(name='prelu1')
layer_factory.new_max_pool(name='pool1', kernel_size=(2, 2), stride_size=(2, 2))
layer_factory.new_conv(name='conv2', kernel_size=(3, 3), channels_output=16, stride_size=(1, 1),
padding='VALID', relu=False)
layer_factory.new_prelu(name='prelu2')
layer_factory.new_conv(name='conv3', kernel_size=(3, 3), channels_output=32, stride_size=(1, 1),
padding='VALID', relu=False)
layer_factory.new_prelu(name='prelu3')
layer_factory.new_conv(name='conv4-1', kernel_size=(1, 1), channels_output=2, stride_size=(1, 1), relu=False)
layer_factory.new_softmax(name='prob1', axis=3)
layer_factory.new_conv(name='conv4-2', kernel_size=(1, 1), channels_output=4, stride_size=(1, 1),
input_layer_name='prelu3', relu=False)
</pre><p><br></p><p>R-Net has a similar structure, but with even more layers. It takes the P-Net bounding boxes as its inputs, and refines its coordinates.</p><p><img src="https://cdn-images-1.medium.com/max/800/1*5KNvVDQHpsquv5yinnTDWw.png"></p><p>Image 6:&nbsp;R-Net</p><p>Similarly, R-Net splits into two layers in the end, giving out two outputs: the coordinates of the new bounding boxes and the machine’s confidence in each bounding box. Again, mtcnn.py includes the structure of R-Net:</p><pre class="ql-syntax" spellcheck="false">class RNet(Network):
def _config(self):
layer_factory = LayerFactory(self)
layer_factory.new_feed(name='data', layer_shape=(None, 24, 24, 3))
layer_factory.new_conv(name='conv1', kernel_size=(3, 3), channels_output=28, stride_size=(1, 1),
padding='VALID', relu=False)
layer_factory.new_prelu(name='prelu1')
layer_factory.new_max_pool(name='pool1', kernel_size=(3, 3), stride_size=(2, 2))
layer_factory.new_conv(name='conv2', kernel_size=(3, 3), channels_output=48, stride_size=(1, 1),
padding='VALID', relu=False)
layer_factory.new_prelu(name='prelu2')
layer_factory.new_max_pool(name='pool2', kernel_size=(3, 3), stride_size=(2, 2), padding='VALID')
layer_factory.new_conv(name='conv3', kernel_size=(2, 2), channels_output=64, stride_size=(1, 1),
padding='VALID', relu=False)
layer_factory.new_prelu(name='prelu3')
layer_factory.new_fully_connected(name='fc1', output_count=128, relu=False)  
layer_factory.new_prelu(name='prelu4')
layer_factory.new_fully_connected(name='fc2-1', output_count=2, relu=False)  
layer_factory.new_softmax(name='prob1', axis=1)
layer_factory.new_fully_connected(name='fc2-2', output_count=4, relu=False, input_layer_name='prelu4')
</pre><p><br></p><p>Finally, O-Net takes the R-Net bounding boxes as inputs and marks down the coordinates of facial landmarks.</p><p><img src="https://cdn-images-1.medium.com/max/800/1*BT6XlTxVjqaNSj87iDFjcg.png"></p><p><img src="https://cdn-images-1.medium.com/max/800/1*eBiydaqk2HU36P0LcUbGzA.png"></p><p>Image 7:&nbsp;O-Net</p><p>O-Net splits into 3 layers in the end, giving out 3 different outputs: the probability of a face being in the box, the coordinates of the bounding box, and the coordinates of the facial landmarks (locations of the eyes, nose, and mouth). Here’s the code for O-Net:</p><pre class="ql-syntax" spellcheck="false">class ONet(Network):
def _config(self):
layer_factory = LayerFactory(self)
layer_factory.new_feed(name='data', layer_shape=(None, 48, 48, 3))
layer_factory.new_conv(name='conv1', kernel_size=(3, 3), channels_output=32, stride_size=(1, 1),
padding='VALID', relu=False)
layer_factory.new_prelu(name='prelu1')
layer_factory.new_max_pool(name='pool1', kernel_size=(3, 3), stride_size=(2, 2))
layer_factory.new_conv(name='conv2', kernel_size=(3, 3), channels_output=64, stride_size=(1, 1),
padding='VALID', relu=False)
layer_factory.new_prelu(name='prelu2')
layer_factory.new_max_pool(name='pool2', kernel_size=(3, 3), stride_size=(2, 2), padding='VALID')
layer_factory.new_conv(name='conv3', kernel_size=(3, 3), channels_output=64, stride_size=(1, 1),
padding='VALID', relu=False)
layer_factory.new_prelu(name='prelu3')
layer_factory.new_max_pool(name='pool3', kernel_size=(2, 2), stride_size=(2, 2))
layer_factory.new_conv(name='conv4', kernel_size=(2, 2), channels_output=128, stride_size=(1, 1),
padding='VALID', relu=False)
layer_factory.new_prelu(name='prelu4')
layer_factory.new_fully_connected(name='fc1', output_count=256, relu=False)
layer_factory.new_prelu(name='prelu5')
layer_factory.new_fully_connected(name='fc2-1', output_count=2, relu=False)
layer_factory.new_softmax(name='prob1', axis=1)
layer_factory.new_fully_connected(name='fc2-2', output_count=4, relu=False, input_layer_name='prelu5')
layer_factory.new_fully_connected(name='fc2-3', output_count=10, relu=False, input_layer_name='prelu5')
</pre><p><br></p><p>Note all the code for P-Net, R-Net, and O-Net all import a class named “LayerFactory”. In essence, LayerFactory is a class — created by the makers of this model — to generate layers with specific settings. For more information, you can check out layer_factory.py.</p><hr class="ql-align-center"><p><br></p><p>Click&nbsp;<a href="https://medium.com/@reina.wang/mtcnn-face-detection-cdcb20448ce0" target="_blank" style="color: inherit; background-color: transparent;">here</a>&nbsp;to read about implementing the MTCNN model!</p><p>Click&nbsp;<a href="https://medium.com/@reina.wang/how-does-a-face-detection-program-work-using-neural-networks-17896df8e6ff" target="_blank" style="color: inherit; background-color: transparent;">here</a>&nbsp;to read about how the MTCNN model works!</p><p><br></p><hr class="ql-align-center"><p><br></p><p>Download the MTCNN paper and resources here:</p><p>Github download:&nbsp;<a href="https://github.com/ipazc/mtcnn" target="_blank" style="color: inherit; background-color: transparent;">https://github.com/ipazc/mtcnn</a></p><p>Research article:&nbsp;<a href="http://arxiv.org/abs/1604.02878" target="_blank" style="color: inherit; background-color: transparent;">http://arxiv.org/abs/1604.02878</a></p><p><br></p><p><br></p><p>===============================================================================================</p><h1>How Does A Face Detection Program Work? (Using Neural Networks)</h1><h2>A Beginner’s Guide to Face Detection With Neural&nbsp;Networks</h2><h2>A Beginner’s Guide to Face Detection With Neural&nbsp;Networks</h2><p><a href="https://towardsdatascience.com/@reina.wang?source=post_header_lockup" target="_blank" style="color: inherit; background-color: transparent;"><img src="https://cdn-images-1.medium.com/fit/c/50/50/1*diRpAT2jWilemimLhfwHzw.jpeg" alt="Go to the profile of Chi-Feng Wang"></a></p><p><a href="https://towardsdatascience.com/@reina.wang" target="_blank" style="color: rgba(0, 0, 0, 0.84); background-color: transparent;">Chi-Feng Wang</a></p><p>Jul 27, 2018</p><p><img src="https://cdn-images-1.medium.com/max/800/1*qOiZiPNaIh-gtjcJJMJRKQ.png"></p><p>Cover image: Face Detection //&nbsp;<a href="https://pixabay.com/en/flat-recognition-facial-face-woman-3252983/" target="_blank" style="color: inherit; background-color: transparent;">Source</a></p><p>Recently, I’ve been playing around with a Multi-task Cascaded Convolutional Network (MTCNN) model for face detection.&nbsp;<a href="http://arxiv.org/abs/1604.02878" target="_blank" style="color: inherit; background-color: transparent;">This model</a>&nbsp;has three convolutional networks (P-Net, R-Net, and O-Net) and is able to outperform many face-detection benchmarks while retaining real-time performance. But how, exactly, does it work?</p><p><strong><em>Note:</em></strong><em>&nbsp;If you want a concrete example of how to process a face detection neural network, I’ve attached the download links of the MTCNN model below. After downloading, open&nbsp;./mtcnn/mtcnn.py and scroll to the detect_faces function. This is the function that you would call when implementing this model, so going through this function would give you a sense of how the program calculates and narrows down the coordinates of the bounding box and facial features. However, I won’t decipher the code line-by-line: not only would it make this article unnecessarily long, it would also be counterproductive for most readers as a lot of the parameters in the code are suited for this specific model only.</em></p><p><img src="https://cdn-images-1.medium.com/max/800/1*ICM3jnRB1unY6G5ZRGorfg.png"></p><p>Image 1: MTCNN Structure //&nbsp;<a href="https://arxiv.org/ftp/arxiv/papers/1604/1604.02878.pdf" target="_blank" style="color: inherit; background-color: transparent;">Source</a></p><hr class="ql-align-center"><p><br></p><h4>Stage 1:</h4><p>Obviously, the first thing to do would be to pass in an image to the program. In this model, we want to create an&nbsp;<strong>image pyramid</strong>, in order to detect faces of all different sizes. In other words, we want to create different copies of the same image in different sizes to search for different sized faces within the image.</p><p><img src="https://cdn-images-1.medium.com/max/800/1*JH-L5EmTqj_fHEcXnzZT5Q.png"></p><p>Image 2: Image Pyramid //&nbsp;<a href="https://arxiv.org/ftp/arxiv/papers/1604/1604.02878.pdf" target="_blank" style="color: inherit; background-color: transparent;">Source</a></p><p>For each scaled copy, we have a 12 x 12 stage 1<strong>&nbsp;kernel</strong>&nbsp;that will go through every part of the image, scanning for faces. It starts in the top left corner, a section of the image from (0,0) to (12,12). This portion of the image is passed to P-Net, which returns the coordinates of a bounding box if it notices a face. Then, it would repeat that process with sections (0+2a,0+2b) to (12+2a, 12+2b), shifting the 12 x 12 kernel 2 pixels right or down at a time. The shift of 2 pixels is known as the&nbsp;<strong>stride</strong>, or how many pixels the kernel moves by every time.</p><p>Having a stride of 2 helps reduce computation complexity without significantly sacrificing accuracy. Since faces in most images are significantly larger than two pixels, it’s highly improbable that the kernel will miss a face merely because it shifted 2 pixels. At the same time, your computer (or whatever machine is running this code) will have a quarter of the amount of operations to compute, making the program run faster and with less memory.</p><p>The only downside is that we have to recalculate all indexes related to the stride. For example, if the kernel detected a face after moving one step to the right, the output index would tell us the top left corner of that kernel is at (1,0). However, because the stride is 2, we have to multiply the index by 2 to get the correct coordinate: (2,0).</p><p>Each kernel would be smaller relative to a large image, so it would be able to find smaller faces in the larger-scaled image. Similarly, the kernel would be bigger relative to a smaller sized image, so it would be able to find bigger faces in the smaller-scaled image.</p><p><br></p><p>After passing in the image, we need to create multiple scaled copies of the image and pass it into the first neural net — P-Net — and gather its output.</p><p><img src="https://cdn-images-1.medium.com/max/800/1*5jhmkluZtYXUbzfi-cplIQ.png"></p><p>Image 3: Sample output for P-Net. Note that the actual output has 4 dimensions, but for simplicity, I’ve combined it into a 2-dimensional array. Also, the coordinates for the bounding boxes are values between 0 and 1: (0,0) would be the top left corner of the kernel, while (1,1) would be the bottom right corner of the&nbsp;kernel.</p><p>The weights and biases of P-Net have been trained so that it outputs a relatively accurate bounding box for every 12 x 12 kernel. However, the network is more confident about some boxes compared to others. Thus, we need to parse the P-Net output to get a list of confidence levels for each bounding box, and delete the boxes with lower confidence (i.e. the boxes that the network isn’t quite sure contains a face)</p><p><img src="https://cdn-images-1.medium.com/max/800/1*Ml1270kY5DoN4vU9Uwwi-A.png"></p><p>Image 4: Standardizing kernel coordinates by multiplying it by the&nbsp;scale</p><p>After we’ve picked out the boxes with higher confidence, we will have to standardize the coordinate system, converting all the coordinate systems to that of the actual, “un-scaled” image. Since most kernels are in a scaled-down image, their coordinates will be based on the smaller image.</p><p>However, there are still a lot of bounding boxes left, and a lot of them overlap.&nbsp;<strong>Non-Maximum Suppression</strong>, or NMS, is a method that reduces the number of bounding boxes.</p><p>In this particular program, NMS is conducted by first sorting the bounding boxes (and their respective 12 x 12 kernels) by their confidence, or score. In some other models, NMS takes the largest bounding box instead of the one the network is most confident in.</p><p>Subsequently, we calculate the area of each of the kernels, as well as the overlapping area between each kernel and the kernel with the highest score. The kernels that overlap a lot with the high-scoring kernel get deleted. Finally, NMS returns a list of the “surviving” bounding boxes.</p><p><img src="https://cdn-images-1.medium.com/max/800/1*cbWsph24bRWIL0JifFVO5A.png"></p><p>Image 5: Non-Maximum Suppression</p><p>We conduct NMS once for every scaled image, then one more time with all the surviving kernels from each scale. This gets rid of redundant bounding boxes, allowing us to narrow our search down to one accurate box per face.</p><p>Why can’t we just choose the box with the highest confidence and delete everything else? There is only one face in the image above. However, there might be more than one face in other images. If so, we would end up deleting all the bounding boxes for the other faces.</p><p>Afterward, we convert the bounding box coordinates to coordinates of the actual image. Right now, the coordinates of each bounding box is a value between 0 and 1, with (0,0) as the top left corner of the 12 x 12 kernel and (1,1) as the bottom right corner (see table above). By multiplying the coordinates by the actual image width and height, we can convert the bounding box coordinates to the standard, real-sized image coordinates.</p><p><img src="https://cdn-images-1.medium.com/max/800/1*7JWQP4FhkMxuCfzSsfFR_A.png"></p><p>Image 6: Here, the red box is the 12 x 12 kernel, while the yellow box is the bounding box inside&nbsp;it.</p><p>In this image, the red box represents the 24 x 24 kernel, resized back to the original image. We can calculate the width and height of the kernel: 1500–200 = 300, 1800–500 = 300 (Note how the width and height aren’t necessarily 12. That is because we’re using the coordinates of the kernel in the original image. The width and height we get here are the width and height of the kernel when scaled back to its original size.) Afterwards, we multiply the bounding box coordinates by 300: 0.4x300 = 120, 0.2x300 = 60, 0.9x300 = 270, 0.7x300 = 210. Finally, we add the top left coordinate of the kernel to get the coordinates of the bounding box: (200+120, 500+60) and (200+270, 500+210) or (320,560) and (470,710).</p><p>Since the bounding boxes may not be square, we then reshape the bounding boxes to a square by elongating the shorter sides (if the width is smaller than the height, we expand it sideways; if the height is smaller than the width, we expand it vertically).</p><p>Finally, we save the coordinates of the bounding boxes and pass it on to stage 2.</p><hr class="ql-align-center"><p><br></p><h4>Stage 2:</h4><p>Sometimes, an image may contain only a part of a face peeking in from the side of the frame. In that case, the network may return a bounding box that is partly out of the frame, like Paul McCartney’s face in the photo below:</p><p><img src="https://cdn-images-1.medium.com/max/800/1*yoZ7DuNCvH64jY6xKFpO7g.png"></p><p>Image 7: The Beatles and their bounding boxes. Paul McCartney’s box is out of bounds and requires padding. //&nbsp;<a href="https://www.flickr.com/photos/beatlesmaniac11/4191790770" target="_blank" style="color: inherit; background-color: transparent;">Source</a></p><p>For every bounding box, we create an array of the same size, and copy the pixel values (the image in the bounding box) to the new array. If the bounding box is out of bounds, we only copy the portion of the image in the bounding box to the new array and fill in everything else with a 0. In the image above, the new array for McCartney’s face would have pixel values in the left side of the box, and several columns of 0s near the right edge. This process of filling arrays with 0s is called&nbsp;<strong>padding</strong>.</p><p>After we pad the bounding box arrays, we resize them to 24 x 24 pixels, and normalize them to values between -1 and 1. Currently, the pixel values are between 0 to 255 (RGB values). By subtracting each pixel value by half of 255 (127.5) and dividing it by 127.5, we can keep their values between -1 and 1.</p><p>Now that we have numerous 24 x 24 image arrays(as many as the number of bounding boxes that survived Stage 1, since each of those bounding boxes has been resized and normalized into these kernels), we can feed them into R-Net and gather its output.</p><p>R-Net’s output is similar to that of P-Net: It includes the coordinates of the new, more accurate bounding boxes, as well as the confidence level of each of these bounding boxes.</p><p>Once again, we get rid of the boxes with lower confidence, and perform NMS on every box to further eliminate redundant boxes. Since the coordinates of these new bounding boxes are based on the P-Net bounding boxes, we need to convert them to the standard coordinates.</p><p>After standardizing the coordinates, we reshape the bounding boxes to a square to be passed on to O-Net.</p><hr class="ql-align-center"><p><br></p><h4>Stage 3:</h4><p>Before we can pass in the bounding boxes from R-Net, we have to first pad any boxes that are out-of-bounds. Then, after we resize the boxes to 48 x 48 pixels, we can pass in the bounding boxes into O-Net.</p><p>The outputs of O-Net are slightly different from that of P-Net and R-Net. O-Net provides 3 outputs: the coordinates of the bounding box (out[0]), the coordinates of the 5 facial landmarks (out[1]), and the confidence level of each box (out[2]).</p><p>Once again, we get rid of the boxes with lower confidence levels, and standardize both the bounding box coordinates and the facial landmark coordinates. Finally, we run them through the last NMS. At this point, there should only be one bounding box for every face in the image.</p><hr class="ql-align-center"><p><br></p><h4><strong>Delivering results:</strong></h4><p>The very last step is to package all the information into a dictionary with three keys: ‘box’, ‘confidence’, and ‘keypoints’. ‘box’ contains the coordinates of the bounding box, ‘confidence’ contains the confidence level of the network for each box, and ‘keypoints’ contains the coordinates of each facial landmark (eyes, nose, and endpoints of the mouth).</p><p>When we want to implement this model in our own code, all we have to do is call detect_faces and we’d be given this dictionary with all the coordinates we need to draw the bounding boxes and mark the facial features.</p><hr class="ql-align-center"><p><br></p><h4>Here’s a short summary of the whole&nbsp;process:</h4><p><strong>Stage 1:</strong></p><p><br></p><ol><li>Pass in image</li><li>Create multiple scaled copies of the image</li><li>Feed scaled images into P-Net</li><li>Gather P-Net output</li><li>Delete bounding boxes with low confidence</li><li>Convert 12 x 12 kernel coordinates to “un-scaled image” coordinates</li><li>Non-Maximum Suppression for kernels in each scaled image</li><li>Non-Maximum Suppression for all kernels</li><li>Convert bounding box coordinates to “un-scaled image” coordinates</li><li>Reshape bounding boxes to square</li></ol><p><strong>Stage 2:</strong></p><ol><li>Pad out-of-bound boxes</li><li>Feed scaled images into R-Net</li><li>Gather R-Net output</li><li>Delete bounding boxes with low confidence</li><li>Non-Maximum Suppression for all boxes</li><li>Convert bounding box coordinates to “un-scaled image” coordinates</li><li>Reshape bounding boxes to square</li></ol><p><strong>Stage 3:</strong></p><ol><li>Pad out-of-bound boxes</li><li>Feed scaled images into O-Net</li><li>Gather O-Net output</li><li>Delete bounding boxes with low confidence</li><li>Convert bounding box and facial landmark coordinates to “un-scaled image” coordinates</li><li>Non-Maximum Suppression for all boxes</li></ol><p><strong>Delivering Results:</strong></p><ol><li>Package all coordinates and confidence levels into a dictionary</li><li>Return the dictionary</li></ol><hr class="ql-align-center"><p><br></p><h4>Final Remarks:</h4><p>This model is rather different from the ones I’ve encountered before. It isn’t just a plain old neural network: it utilizes some interesting techniques to achieve high accuracy with less run-time.</p><p>Low computation complexity results in a fast run-time. To achieve real-time performance, it uses a stride of 2, reducing the number of operations to a quarter of the original. It also doesn’t start finding facial landmarks until the last network (O-Net), after the bounding box coordinates have been calibrated, which narrows down the coordinates of facial features even before it begins its calculations. This makes the program a lot faster as it only has to find facial landmarks in the few boxes that pass through O-Net.</p><p>High accuracy is achieved with a deep neural network. Having 3 networks — each with multiple layers — allows for higher precision, as each network can fine-tune the results of the previous one. In addition, this model employs an image pyramid to find faces both large and small. Even though this may produce an overwhelming amount of data, NMS, as well as R-Net and O-Net, all help reject a large number of false bounding boxes.</p><p>Looking at this model’s high performance, I can’t help but wonder what else we can utilize this model for. Would it be able to recognize certain animals? Certain cars? Can it be adjusted to do facial recognition as well as facial detection? This model — and its applications — gave us countless applications for future use.</p><hr class="ql-align-center"><p><br></p><ul><li>Click&nbsp;<a href="https://medium.com/@reina.wang/mtcnn-face-detection-cdcb20448ce0" target="_blank" style="color: inherit; background-color: transparent;">here</a>&nbsp;to read about implementing the MTCNN model!</li><li>Click&nbsp;<a href="https://medium.com/@reina.wang/face-detection-neural-network-structure-257b8f6f85d1" target="_blank" style="color: inherit; background-color: transparent;">here</a>&nbsp;to read about the network structure of the MTCNN model!</li></ul><p><br></p><hr class="ql-align-center"><p><br></p><p>Download the MTCNN paper and resources here:</p><ul><li>Github download:<a href="https://github.com/ipazc/mtcnn" target="_blank" style="color: inherit; background-color: transparent;">&nbsp;https://github.com/ipazc/mtcnn</a></li><li>Research article:&nbsp;<a href="http://arxiv.org/abs/1604.02878" target="_blank" style="color: inherit; background-color: transparent;">http://arxiv.org/abs/1604.02878</a></li></ul><p><br></p><p>=======================================================================================================</p><p><br></p><p><br></p>