<attachment contenteditable="false" data-atts="%5B%5D" data-aid=".atts-0cbe78bd-1b13-468c-ad4b-608d34bdf83b"></attachment><p>tensorflow 迁移学习：<a href="https://codelabs.developers.google.com/codelabs/tensorflow-for-poets/#0" target="_blank">https://codelabs.developers.google.com/codelabs/tensorflow-for-poets/#0</a></p><p><br></p><h1>What Does A Face Detection Neural Network Look&nbsp;Like?</h1><p><a href="https://towardsdatascience.com/@reina.wang?source=post_header_lockup" target="_blank" style="color: inherit; background-color: transparent;"><img src="https://cdn-images-1.medium.com/fit/c/50/50/1*diRpAT2jWilemimLhfwHzw.jpeg" alt="Go to the profile of Chi-Feng Wang"></a></p><p><a href="https://towardsdatascience.com/@reina.wang" target="_blank" style="color: rgba(0, 0, 0, 0.84); background-color: transparent;">Chi-Feng Wang</a></p><p>Jul 24, 2018</p><p>In my&nbsp;<a href="https://medium.com/@reina.wang/mtcnn-face-detection-cdcb20448ce0" target="_blank" style="color: inherit; background-color: transparent;">last post</a>, I explored the Multi-task Cascaded Convolutional Network (MTCNN) model, using it to detect faces with my webcam. In this post, I will examine the structure of the neural network.</p><p>The MTCNN model consists of 3 separate networks: the P-Net, the R-Net, and the O-Net:</p><p><img src="https://cdn-images-1.medium.com/max/800/1*ICM3jnRB1unY6G5ZRGorfg.png"></p><p>Image 1: MTCNN Structure //&nbsp;<a href="https://arxiv.org/ftp/arxiv/papers/1604/1604.02878.pdf" target="_blank" style="color: inherit; background-color: transparent;">Source</a></p><p>For every image we pass in, the network creates an image pyramid: that is, it creates multiple copies of that image in different sizes.</p><p><img src="https://cdn-images-1.medium.com/max/800/1*JH-L5EmTqj_fHEcXnzZT5Q.png"></p><p>Image 2: Image Pyramid //&nbsp;<a href="https://arxiv.org/ftp/arxiv/papers/1604/1604.02878.pdf" target="_blank" style="color: inherit; background-color: transparent;">Source</a></p><p>In the P-Net, for each scaled image, a 12x12 kernel runs through the image, searching for a face. In the image below, the red square represents the kernel, which slowly moves across and down the image, searching for a face.</p><p><img src="https://cdn-images-1.medium.com/max/800/1*r3aAaOV2CWYBJav3uWRP5A.png"></p><p>Image 3: 12x12 kernel in the top right corner. After scanning this corner, it shifts sideways (or downwards) by 1 pixel, and continues doing that until it has gone through the entire&nbsp;image.</p><p>Within each of these 12x12 kernels, 3 convolutions are run through (If you don’t know what convolutions are, check out&nbsp;<a href="https://medium.com/@reina.wang.tw/what-is-a-neural-network-6010edabde2b" target="_blank" style="color: inherit; background-color: transparent;">my other article</a>&nbsp;or&nbsp;<a href="http://setosa.io/ev/image-kernels/" target="_blank" style="color: inherit; background-color: transparent;">this site</a>) with 3x3 kernels. After every convolution layer, a prelu layer is implemented (when you multiply every negative pixel with a certain number ‘alpha’. ‘Alpha’ is to be determined through training). In addition, a maxpool layer is put in after the first prelu layer(maxpool takes out every other pixel, leaving only the largest one in the vicinity).</p><p><img src="https://cdn-images-1.medium.com/max/800/1*w49KKbft4Iq3xLsoNy-l-Q.png"></p><p>Image 4: Max-pool //&nbsp;<a href="https://youtu.be/gbceqO8PpBg" target="_blank" style="color: inherit; background-color: transparent;">Source</a></p><p>After the third convolution layer, the network splits into two layers. The activations from the third layer are passed to two separate convolution layers, and a softmax layer after one of those convolution layers (softmax assigns decimal probabilities to every result, and the probabilities add up to 1. In this case, it outputs 2 probabilities: the probability that there&nbsp;<strong>is</strong>&nbsp;a face in the area and the probability that there&nbsp;<strong>isn’t&nbsp;</strong>a face).</p><p><img src="https://cdn-images-1.medium.com/max/800/1*Ey4E1ZreYY8F_GiXLVCD_Q.png"></p><p>Image 5:&nbsp;P-Net</p><p>Convolution 4–1 outputs the probability of a face being in each bounding box, and convolution 4–2 outputs the coordinates of the bounding boxes.</p><p>Taking a look at mtcnn.py will show you the structure of P-Net:</p><pre class="ql-syntax" spellcheck="false">class PNet(Network):
def _config(self):
layer_factory = LayerFactory(self)
layer_factory.new_feed(name='data', layer_shape=(None, None, None, 3))
layer_factory.new_conv(name='conv1', kernel_size=(3, 3), channels_output=10, stride_size=(1, 1),
padding='VALID', relu=False)
layer_factory.new_prelu(name='prelu1')
layer_factory.new_max_pool(name='pool1', kernel_size=(2, 2), stride_size=(2, 2))
layer_factory.new_conv(name='conv2', kernel_size=(3, 3), channels_output=16, stride_size=(1, 1),
padding='VALID', relu=False)
layer_factory.new_prelu(name='prelu2')
layer_factory.new_conv(name='conv3', kernel_size=(3, 3), channels_output=32, stride_size=(1, 1),
padding='VALID', relu=False)
layer_factory.new_prelu(name='prelu3')
layer_factory.new_conv(name='conv4-1', kernel_size=(1, 1), channels_output=2, stride_size=(1, 1), relu=False)
layer_factory.new_softmax(name='prob1', axis=3)
layer_factory.new_conv(name='conv4-2', kernel_size=(1, 1), channels_output=4, stride_size=(1, 1),
input_layer_name='prelu3', relu=False)
</pre><p><br></p><p>R-Net has a similar structure, but with even more layers. It takes the P-Net bounding boxes as its inputs, and refines its coordinates.</p><p><img src="https://cdn-images-1.medium.com/max/800/1*5KNvVDQHpsquv5yinnTDWw.png"></p><p>Image 6:&nbsp;R-Net</p><p>Similarly, R-Net splits into two layers in the end, giving out two outputs: the coordinates of the new bounding boxes and the machine’s confidence in each bounding box. Again, mtcnn.py includes the structure of R-Net:</p><pre class="ql-syntax" spellcheck="false">class RNet(Network):
def _config(self):
layer_factory = LayerFactory(self)
layer_factory.new_feed(name='data', layer_shape=(None, 24, 24, 3))
layer_factory.new_conv(name='conv1', kernel_size=(3, 3), channels_output=28, stride_size=(1, 1),
padding='VALID', relu=False)
layer_factory.new_prelu(name='prelu1')
layer_factory.new_max_pool(name='pool1', kernel_size=(3, 3), stride_size=(2, 2))
layer_factory.new_conv(name='conv2', kernel_size=(3, 3), channels_output=48, stride_size=(1, 1),
padding='VALID', relu=False)
layer_factory.new_prelu(name='prelu2')
layer_factory.new_max_pool(name='pool2', kernel_size=(3, 3), stride_size=(2, 2), padding='VALID')
layer_factory.new_conv(name='conv3', kernel_size=(2, 2), channels_output=64, stride_size=(1, 1),
padding='VALID', relu=False)
layer_factory.new_prelu(name='prelu3')
layer_factory.new_fully_connected(name='fc1', output_count=128, relu=False)  
layer_factory.new_prelu(name='prelu4')
layer_factory.new_fully_connected(name='fc2-1', output_count=2, relu=False)  
layer_factory.new_softmax(name='prob1', axis=1)
layer_factory.new_fully_connected(name='fc2-2', output_count=4, relu=False, input_layer_name='prelu4')
</pre><p><br></p><p>Finally, O-Net takes the R-Net bounding boxes as inputs and marks down the coordinates of facial landmarks.</p><p><img src="https://cdn-images-1.medium.com/max/800/1*BT6XlTxVjqaNSj87iDFjcg.png"></p><p><img src="https://cdn-images-1.medium.com/max/800/1*eBiydaqk2HU36P0LcUbGzA.png"></p><p>Image 7:&nbsp;O-Net</p><p>O-Net splits into 3 layers in the end, giving out 3 different outputs: the probability of a face being in the box, the coordinates of the bounding box, and the coordinates of the facial landmarks (locations of the eyes, nose, and mouth). Here’s the code for O-Net:</p><pre class="ql-syntax" spellcheck="false">class ONet(Network):
def _config(self):
layer_factory = LayerFactory(self)
layer_factory.new_feed(name='data', layer_shape=(None, 48, 48, 3))
layer_factory.new_conv(name='conv1', kernel_size=(3, 3), channels_output=32, stride_size=(1, 1),
padding='VALID', relu=False)
layer_factory.new_prelu(name='prelu1')
layer_factory.new_max_pool(name='pool1', kernel_size=(3, 3), stride_size=(2, 2))
layer_factory.new_conv(name='conv2', kernel_size=(3, 3), channels_output=64, stride_size=(1, 1),
padding='VALID', relu=False)
layer_factory.new_prelu(name='prelu2')
layer_factory.new_max_pool(name='pool2', kernel_size=(3, 3), stride_size=(2, 2), padding='VALID')
layer_factory.new_conv(name='conv3', kernel_size=(3, 3), channels_output=64, stride_size=(1, 1),
padding='VALID', relu=False)
layer_factory.new_prelu(name='prelu3')
layer_factory.new_max_pool(name='pool3', kernel_size=(2, 2), stride_size=(2, 2))
layer_factory.new_conv(name='conv4', kernel_size=(2, 2), channels_output=128, stride_size=(1, 1),
padding='VALID', relu=False)
layer_factory.new_prelu(name='prelu4')
layer_factory.new_fully_connected(name='fc1', output_count=256, relu=False)
layer_factory.new_prelu(name='prelu5')
layer_factory.new_fully_connected(name='fc2-1', output_count=2, relu=False)
layer_factory.new_softmax(name='prob1', axis=1)
layer_factory.new_fully_connected(name='fc2-2', output_count=4, relu=False, input_layer_name='prelu5')
layer_factory.new_fully_connected(name='fc2-3', output_count=10, relu=False, input_layer_name='prelu5')
</pre><p><br></p><p>Note all the code for P-Net, R-Net, and O-Net all import a class named “LayerFactory”. In essence, LayerFactory is a class — created by the makers of this model — to generate layers with specific settings. For more information, you can check out layer_factory.py.</p><hr class="ql-align-center"><p><br></p><p>Click&nbsp;<a href="https://medium.com/@reina.wang/mtcnn-face-detection-cdcb20448ce0" target="_blank" style="color: inherit; background-color: transparent;">here</a>&nbsp;to read about implementing the MTCNN model!</p><p>Click&nbsp;<a href="https://medium.com/@reina.wang/how-does-a-face-detection-program-work-using-neural-networks-17896df8e6ff" target="_blank" style="color: inherit; background-color: transparent;">here</a>&nbsp;to read about how the MTCNN model works!</p><p><br></p><hr class="ql-align-center"><p><br></p><p>Download the MTCNN paper and resources here:</p><p>Github download:&nbsp;<a href="https://github.com/ipazc/mtcnn" target="_blank" style="color: inherit; background-color: transparent;">https://github.com/ipazc/mtcnn</a></p><p>Research article:&nbsp;<a href="http://arxiv.org/abs/1604.02878" target="_blank" style="color: inherit; background-color: transparent;">http://arxiv.org/abs/1604.02878</a></p><p><br></p><p><br></p><p>===============================================================================================</p><h1>How Does A Face Detection Program Work? (Using Neural Networks)</h1><h2>A Beginner’s Guide to Face Detection With Neural&nbsp;Networks</h2><h2>A Beginner’s Guide to Face Detection With Neural&nbsp;Networks</h2><p><a href="https://towardsdatascience.com/@reina.wang?source=post_header_lockup" target="_blank" style="color: inherit; background-color: transparent;"><img src="https://cdn-images-1.medium.com/fit/c/50/50/1*diRpAT2jWilemimLhfwHzw.jpeg" alt="Go to the profile of Chi-Feng Wang"></a></p><p><a href="https://towardsdatascience.com/@reina.wang" target="_blank" style="color: rgba(0, 0, 0, 0.84); background-color: transparent;">Chi-Feng Wang</a></p><p>Jul 27, 2018</p><p><img src="https://cdn-images-1.medium.com/max/800/1*qOiZiPNaIh-gtjcJJMJRKQ.png"></p><p>Cover image: Face Detection //&nbsp;<a href="https://pixabay.com/en/flat-recognition-facial-face-woman-3252983/" target="_blank" style="color: inherit; background-color: transparent;">Source</a></p><p>Recently, I’ve been playing around with a Multi-task Cascaded Convolutional Network (MTCNN) model for face detection.&nbsp;<a href="http://arxiv.org/abs/1604.02878" target="_blank" style="color: inherit; background-color: transparent;">This model</a>&nbsp;has three convolutional networks (P-Net, R-Net, and O-Net) and is able to outperform many face-detection benchmarks while retaining real-time performance. But how, exactly, does it work?</p><p><strong><em>Note:</em></strong><em>&nbsp;If you want a concrete example of how to process a face detection neural network, I’ve attached the download links of the MTCNN model below. After downloading, open&nbsp;./mtcnn/mtcnn.py and scroll to the detect_faces function. This is the function that you would call when implementing this model, so going through this function would give you a sense of how the program calculates and narrows down the coordinates of the bounding box and facial features. However, I won’t decipher the code line-by-line: not only would it make this article unnecessarily long, it would also be counterproductive for most readers as a lot of the parameters in the code are suited for this specific model only.</em></p><p><img src="https://cdn-images-1.medium.com/max/800/1*ICM3jnRB1unY6G5ZRGorfg.png"></p><p>Image 1: MTCNN Structure //&nbsp;<a href="https://arxiv.org/ftp/arxiv/papers/1604/1604.02878.pdf" target="_blank" style="color: inherit; background-color: transparent;">Source</a></p><hr class="ql-align-center"><p><br></p><h4>Stage 1:</h4><p>Obviously, the first thing to do would be to pass in an image to the program. In this model, we want to create an&nbsp;<strong>image pyramid</strong>, in order to detect faces of all different sizes. In other words, we want to create different copies of the same image in different sizes to search for different sized faces within the image.</p><p><img src="https://cdn-images-1.medium.com/max/800/1*JH-L5EmTqj_fHEcXnzZT5Q.png"></p><p>Image 2: Image Pyramid //&nbsp;<a href="https://arxiv.org/ftp/arxiv/papers/1604/1604.02878.pdf" target="_blank" style="color: inherit; background-color: transparent;">Source</a></p><p>For each scaled copy, we have a 12 x 12 stage 1<strong>&nbsp;kernel</strong>&nbsp;that will go through every part of the image, scanning for faces. It starts in the top left corner, a section of the image from (0,0) to (12,12). This portion of the image is passed to P-Net, which returns the coordinates of a bounding box if it notices a face. Then, it would repeat that process with sections (0+2a,0+2b) to (12+2a, 12+2b), shifting the 12 x 12 kernel 2 pixels right or down at a time. The shift of 2 pixels is known as the&nbsp;<strong>stride</strong>, or how many pixels the kernel moves by every time.</p><p>Having a stride of 2 helps reduce computation complexity without significantly sacrificing accuracy. Since faces in most images are significantly larger than two pixels, it’s highly improbable that the kernel will miss a face merely because it shifted 2 pixels. At the same time, your computer (or whatever machine is running this code) will have a quarter of the amount of operations to compute, making the program run faster and with less memory.</p><p>The only downside is that we have to recalculate all indexes related to the stride. For example, if the kernel detected a face after moving one step to the right, the output index would tell us the top left corner of that kernel is at (1,0). However, because the stride is 2, we have to multiply the index by 2 to get the correct coordinate: (2,0).</p><p>Each kernel would be smaller relative to a large image, so it would be able to find smaller faces in the larger-scaled image. Similarly, the kernel would be bigger relative to a smaller sized image, so it would be able to find bigger faces in the smaller-scaled image.</p><p><br></p><p>After passing in the image, we need to create multiple scaled copies of the image and pass it into the first neural net — P-Net — and gather its output.</p><p><img src="https://cdn-images-1.medium.com/max/800/1*5jhmkluZtYXUbzfi-cplIQ.png"></p><p>Image 3: Sample output for P-Net. Note that the actual output has 4 dimensions, but for simplicity, I’ve combined it into a 2-dimensional array. Also, the coordinates for the bounding boxes are values between 0 and 1: (0,0) would be the top left corner of the kernel, while (1,1) would be the bottom right corner of the&nbsp;kernel.</p><p>The weights and biases of P-Net have been trained so that it outputs a relatively accurate bounding box for every 12 x 12 kernel. However, the network is more confident about some boxes compared to others. Thus, we need to parse the P-Net output to get a list of confidence levels for each bounding box, and delete the boxes with lower confidence (i.e. the boxes that the network isn’t quite sure contains a face)</p><p><img src="https://cdn-images-1.medium.com/max/800/1*Ml1270kY5DoN4vU9Uwwi-A.png"></p><p>Image 4: Standardizing kernel coordinates by multiplying it by the&nbsp;scale</p><p>After we’ve picked out the boxes with higher confidence, we will have to standardize the coordinate system, converting all the coordinate systems to that of the actual, “un-scaled” image. Since most kernels are in a scaled-down image, their coordinates will be based on the smaller image.</p><p>However, there are still a lot of bounding boxes left, and a lot of them overlap.&nbsp;<strong>Non-Maximum Suppression</strong>, or NMS, is a method that reduces the number of bounding boxes.</p><p>In this particular program, NMS is conducted by first sorting the bounding boxes (and their respective 12 x 12 kernels) by their confidence, or score. In some other models, NMS takes the largest bounding box instead of the one the network is most confident in.</p><p>Subsequently, we calculate the area of each of the kernels, as well as the overlapping area between each kernel and the kernel with the highest score. The kernels that overlap a lot with the high-scoring kernel get deleted. Finally, NMS returns a list of the “surviving” bounding boxes.</p><p><img src="https://cdn-images-1.medium.com/max/800/1*cbWsph24bRWIL0JifFVO5A.png"></p><p>Image 5: Non-Maximum Suppression</p><p>We conduct NMS once for every scaled image, then one more time with all the surviving kernels from each scale. This gets rid of redundant bounding boxes, allowing us to narrow our search down to one accurate box per face.</p><p>Why can’t we just choose the box with the highest confidence and delete everything else? There is only one face in the image above. However, there might be more than one face in other images. If so, we would end up deleting all the bounding boxes for the other faces.</p><p>Afterward, we convert the bounding box coordinates to coordinates of the actual image. Right now, the coordinates of each bounding box is a value between 0 and 1, with (0,0) as the top left corner of the 12 x 12 kernel and (1,1) as the bottom right corner (see table above). By multiplying the coordinates by the actual image width and height, we can convert the bounding box coordinates to the standard, real-sized image coordinates.</p><p><img src="https://cdn-images-1.medium.com/max/800/1*7JWQP4FhkMxuCfzSsfFR_A.png"></p><p>Image 6: Here, the red box is the 12 x 12 kernel, while the yellow box is the bounding box inside&nbsp;it.</p><p>In this image, the red box represents the 24 x 24 kernel, resized back to the original image. We can calculate the width and height of the kernel: 1500–200 = 300, 1800–500 = 300 (Note how the width and height aren’t necessarily 12. That is because we’re using the coordinates of the kernel in the original image. The width and height we get here are the width and height of the kernel when scaled back to its original size.) Afterwards, we multiply the bounding box coordinates by 300: 0.4x300 = 120, 0.2x300 = 60, 0.9x300 = 270, 0.7x300 = 210. Finally, we add the top left coordinate of the kernel to get the coordinates of the bounding box: (200+120, 500+60) and (200+270, 500+210) or (320,560) and (470,710).</p><p>Since the bounding boxes may not be square, we then reshape the bounding boxes to a square by elongating the shorter sides (if the width is smaller than the height, we expand it sideways; if the height is smaller than the width, we expand it vertically).</p><p>Finally, we save the coordinates of the bounding boxes and pass it on to stage 2.</p><hr class="ql-align-center"><p><br></p><h4>Stage 2:</h4><p>Sometimes, an image may contain only a part of a face peeking in from the side of the frame. In that case, the network may return a bounding box that is partly out of the frame, like Paul McCartney’s face in the photo below:</p><p><img src="https://cdn-images-1.medium.com/max/800/1*yoZ7DuNCvH64jY6xKFpO7g.png"></p><p>Image 7: The Beatles and their bounding boxes. Paul McCartney’s box is out of bounds and requires padding. //&nbsp;<a href="https://www.flickr.com/photos/beatlesmaniac11/4191790770" target="_blank" style="color: inherit; background-color: transparent;">Source</a></p><p>For every bounding box, we create an array of the same size, and copy the pixel values (the image in the bounding box) to the new array. If the bounding box is out of bounds, we only copy the portion of the image in the bounding box to the new array and fill in everything else with a 0. In the image above, the new array for McCartney’s face would have pixel values in the left side of the box, and several columns of 0s near the right edge. This process of filling arrays with 0s is called&nbsp;<strong>padding</strong>.</p><p>After we pad the bounding box arrays, we resize them to 24 x 24 pixels, and normalize them to values between -1 and 1. Currently, the pixel values are between 0 to 255 (RGB values). By subtracting each pixel value by half of 255 (127.5) and dividing it by 127.5, we can keep their values between -1 and 1.</p><p>Now that we have numerous 24 x 24 image arrays(as many as the number of bounding boxes that survived Stage 1, since each of those bounding boxes has been resized and normalized into these kernels), we can feed them into R-Net and gather its output.</p><p>R-Net’s output is similar to that of P-Net: It includes the coordinates of the new, more accurate bounding boxes, as well as the confidence level of each of these bounding boxes.</p><p>Once again, we get rid of the boxes with lower confidence, and perform NMS on every box to further eliminate redundant boxes. Since the coordinates of these new bounding boxes are based on the P-Net bounding boxes, we need to convert them to the standard coordinates.</p><p>After standardizing the coordinates, we reshape the bounding boxes to a square to be passed on to O-Net.</p><hr class="ql-align-center"><p><br></p><h4>Stage 3:</h4><p>Before we can pass in the bounding boxes from R-Net, we have to first pad any boxes that are out-of-bounds. Then, after we resize the boxes to 48 x 48 pixels, we can pass in the bounding boxes into O-Net.</p><p>The outputs of O-Net are slightly different from that of P-Net and R-Net. O-Net provides 3 outputs: the coordinates of the bounding box (out[0]), the coordinates of the 5 facial landmarks (out[1]), and the confidence level of each box (out[2]).</p><p>Once again, we get rid of the boxes with lower confidence levels, and standardize both the bounding box coordinates and the facial landmark coordinates. Finally, we run them through the last NMS. At this point, there should only be one bounding box for every face in the image.</p><hr class="ql-align-center"><p><br></p><h4><strong>Delivering results:</strong></h4><p>The very last step is to package all the information into a dictionary with three keys: ‘box’, ‘confidence’, and ‘keypoints’. ‘box’ contains the coordinates of the bounding box, ‘confidence’ contains the confidence level of the network for each box, and ‘keypoints’ contains the coordinates of each facial landmark (eyes, nose, and endpoints of the mouth).</p><p>When we want to implement this model in our own code, all we have to do is call detect_faces and we’d be given this dictionary with all the coordinates we need to draw the bounding boxes and mark the facial features.</p><hr class="ql-align-center"><p><br></p><h4>Here’s a short summary of the whole&nbsp;process:</h4><p><strong>Stage 1:</strong></p><p><br></p><ol><li>Pass in image</li><li>Create multiple scaled copies of the image</li><li>Feed scaled images into P-Net</li><li>Gather P-Net output</li><li>Delete bounding boxes with low confidence</li><li>Convert 12 x 12 kernel coordinates to “un-scaled image” coordinates</li><li>Non-Maximum Suppression for kernels in each scaled image</li><li>Non-Maximum Suppression for all kernels</li><li>Convert bounding box coordinates to “un-scaled image” coordinates</li><li>Reshape bounding boxes to square</li></ol><p><strong>Stage 2:</strong></p><ol><li>Pad out-of-bound boxes</li><li>Feed scaled images into R-Net</li><li>Gather R-Net output</li><li>Delete bounding boxes with low confidence</li><li>Non-Maximum Suppression for all boxes</li><li>Convert bounding box coordinates to “un-scaled image” coordinates</li><li>Reshape bounding boxes to square</li></ol><p><strong>Stage 3:</strong></p><ol><li>Pad out-of-bound boxes</li><li>Feed scaled images into O-Net</li><li>Gather O-Net output</li><li>Delete bounding boxes with low confidence</li><li>Convert bounding box and facial landmark coordinates to “un-scaled image” coordinates</li><li>Non-Maximum Suppression for all boxes</li></ol><p><strong>Delivering Results:</strong></p><ol><li>Package all coordinates and confidence levels into a dictionary</li><li>Return the dictionary</li></ol><hr class="ql-align-center"><p><br></p><h4>Final Remarks:</h4><p>This model is rather different from the ones I’ve encountered before. It isn’t just a plain old neural network: it utilizes some interesting techniques to achieve high accuracy with less run-time.</p><p>Low computation complexity results in a fast run-time. To achieve real-time performance, it uses a stride of 2, reducing the number of operations to a quarter of the original. It also doesn’t start finding facial landmarks until the last network (O-Net), after the bounding box coordinates have been calibrated, which narrows down the coordinates of facial features even before it begins its calculations. This makes the program a lot faster as it only has to find facial landmarks in the few boxes that pass through O-Net.</p><p>High accuracy is achieved with a deep neural network. Having 3 networks — each with multiple layers — allows for higher precision, as each network can fine-tune the results of the previous one. In addition, this model employs an image pyramid to find faces both large and small. Even though this may produce an overwhelming amount of data, NMS, as well as R-Net and O-Net, all help reject a large number of false bounding boxes.</p><p>Looking at this model’s high performance, I can’t help but wonder what else we can utilize this model for. Would it be able to recognize certain animals? Certain cars? Can it be adjusted to do facial recognition as well as facial detection? This model — and its applications — gave us countless applications for future use.</p><hr class="ql-align-center"><p><br></p><ul><li>Click&nbsp;<a href="https://medium.com/@reina.wang/mtcnn-face-detection-cdcb20448ce0" target="_blank" style="color: inherit; background-color: transparent;">here</a>&nbsp;to read about implementing the MTCNN model!</li><li>Click&nbsp;<a href="https://medium.com/@reina.wang/face-detection-neural-network-structure-257b8f6f85d1" target="_blank" style="color: inherit; background-color: transparent;">here</a>&nbsp;to read about the network structure of the MTCNN model!</li></ul><p><br></p><hr class="ql-align-center"><p><br></p><p>Download the MTCNN paper and resources here:</p><ul><li>Github download:<a href="https://github.com/ipazc/mtcnn" target="_blank" style="color: inherit; background-color: transparent;">&nbsp;https://github.com/ipazc/mtcnn</a></li><li>Research article:&nbsp;<a href="http://arxiv.org/abs/1604.02878" target="_blank" style="color: inherit; background-color: transparent;">http://arxiv.org/abs/1604.02878</a></li></ul><p><br></p><p>=======================================================================================================</p><h1>How Do You Train a Face Detection Model?</h1><p><a href="https://towardsdatascience.com/@reina.wang?source=post_header_lockup" target="_blank" style="color: inherit; background-color: transparent;"><span><img src="https://cdn-images-1.medium.com/fit/c/50/50/1*diRpAT2jWilemimLhfwHzw.jpeg" alt="Go to the profile of Chi-Feng Wang"></span></a></p><p><a href="https://towardsdatascience.com/@reina.wang" target="_blank" style="color: rgba(0, 0, 0, 0.84); background-color: transparent;"><span>Chi-Feng Wang</span></a></p><p>Aug 2, 2018</p><p>As I’ve been exploring the&nbsp;<a href="https://github.com/ipazc/mtcnn" target="_blank" style="color: inherit; background-color: transparent;"><span>MTCNN model</span></a><span>&nbsp;(read more about it&nbsp;</span><a href="https://towardsdatascience.com/how-does-a-face-detection-program-work-using-neural-networks-17896df8e6ff" target="_blank" style="color: inherit; background-color: transparent;"><span>here</span></a><span>) so much recently, I decided to try training it. Even just thinking about it conceptually, training the MTCNN model was a challenge. It is a cascaded convolutional network, meaning it is composed of 3 separate neural networks that couldn’t be trained together. I decided to start by training P-Net, the first network.</span></p><p></p><p><span>P-Net is your traditional 12-Net: It takes a 12x12 pixel image as an input and outputs a matrix result telling you whether or not a there is a face — and if there is, the coordinates of the bounding boxes and facial landmarks for each face. Therefore, I had to start by creating a dataset composed solely of 12x12 pixel images.</span></p><p><span>The team that developed this model used the&nbsp;</span><a href="http://mmlab.ie.cuhk.edu.hk/projects/WIDERFace/" target="_blank" style="color: inherit; background-color: transparent;"><span>WIDER-FACE dataset</span></a><span>&nbsp;to train bounding box coordinates and the&nbsp;</span><a href="http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html" target="_blank" style="color: inherit; background-color: transparent;"><span>CelebA dataset</span></a><span>&nbsp;to train facial landmarks. For simplicity’s sake, I started by training only the bounding box coordinates.</span></p><p><span>The WIDER-FACE dataset includes&nbsp;</span><strong><span>32,203</span></strong><span>&nbsp;images with&nbsp;</span><strong><span>393,703</span></strong><span>&nbsp;faces of people in different situations. These images were split into a training set, a validation set, and a testing set. Under the training set, the images were split by occasion:</span></p><p><span><img src="https://cdn-images-1.medium.com/max/800/1*DDE3E4ll8o7L4TjvOYu1RA.png"></span></p><p>Image 1: Folders of different images</p><p>Inside each folder were hundreds of photos with thousands of faces:</p><p><span><img src="https://cdn-images-1.medium.com/max/800/1*ssc2364s2dfP_3Lir6yLYA.png"></span></p><p>Image 2: Inside a&nbsp;folder</p><p>All these photos, however, were significantly larger than 12x12 pixels. I had to crop each of them into multiple 12x12 squares, some of which contained faces and some of which don’t.</p><p>I considered simply creating a 12x12 kernel that moved across each image and copied the image within it every 2 pixels it moved. However, that would leave me with millions of photos, most of which don’t contain faces. To ensure a better training process, I wanted about 50% of my training photos to contain a face. In addition, faces could be of different sizes. I needed images of different sized faces.</p><p>This was what I decided to do: First, I would load in the photos, getting rid of any photo with more than one face as those only made the cropping process more complicated.</p><iframe class="ql-video" frameborder="0" allowfullscreen="true" src="https://towardsdatascience.com/media/d58f681da0e2f0d6aec7ec302a428505?postId=a60330f15fd5" height="250" width="700"></iframe><p></p><p>Then, I’ll create 4 different scaled copies of each photo, so that I have one copy where the face in the photo is 12 pixels tall, one where it’s 11 pixels tall, one where it’s 10 pixels tall, and one where it’s 9 pixels tall.</p><iframe class="ql-video" frameborder="0" allowfullscreen="true" src="https://towardsdatascience.com/media/d1c617f97a9d981971ffdcbfb35260e0?postId=a60330f15fd5" height="250" width="700"></iframe><p></p><p>A face smaller than 9x9 pixels is too small to be recognized. To illustrate my point, here’s a 9x9 pixel image of young Justin Bieber’s face:</p><p><img src="https://cdn-images-1.medium.com/max/800/1*Wz6R26oof7TYjedRtR5r8Q.jpeg"></p><p></p><p>Image 3: 9x9 Justin Bieber //&nbsp;<a href="https://commons.wikimedia.org/wiki/File:Justin_Bieber_at_Easter_Egg_roll.jpg" target="_blank" style="color: inherit; background-color: transparent;"><span>Source</span></a></p><p>For each scaled copy, I’ll crop as many 12x12 pixel images as I can. For example, in this 12x11 pixel image of Justin Bieber, I can crop 2 images with his face in it.</p><iframe class="ql-video" frameborder="0" allowfullscreen="true" src="https://towardsdatascience.com/media/68f9c063ca757863b84223ae3dfa3dc2?postId=a60330f15fd5" height="393" width="700"></iframe><p><br></p><p></p><p>With the smaller scales, I can crop even more 12x12 images. For each cropped image, I need to convert the bounding box coordinates of a value between 0 and 1, where the top left corner of the image is (0,0) and the bottom right is (1,1). This makes it easier to handle calculations and scale images and bounding boxes back to their original size.</p><p><span><img src="https://cdn-images-1.medium.com/max/800/1*HIC6kPmGelZwzudYm5G5XA.png"></span></p><p>Image 4: Converting bounding box coordinates //&nbsp;<a href="https://commons.wikimedia.org/wiki/File:Justin_Bieber_at_Easter_Egg_roll.jpg" target="_blank" style="color: inherit; background-color: transparent;"><span>Source</span></a></p><p>Finally, I saved the bounding box coordinates into a&nbsp;.txt file. I ran that a few times, and found that each face produced approximately 60 cropped images. All I need to do is just create 60 more cropped images with no face in them.</p><p>Generating negative (no-face) images is easier than generating positive (with face) images. Similarly, I created multiple scaled copies of each image with faces 12, 11, 10, and 9 pixels tall, then I randomly drew 12x12 pixel boxes. If that box happened to land within the bounding box, I drew another one. If the box did not overlap with the bounding box, I cropped that portion of the image.</p><iframe class="ql-video" frameborder="0" allowfullscreen="true" src="https://towardsdatascience.com/media/faef81161c57008c34640224f543181b?postId=a60330f15fd5" height="250" width="700"></iframe><p></p><p>In the end, I generated around 5000 positive and 5000 negative images. That’s enough to do a very simple, short training.</p><hr class="ql-align-center"><p></p><p>Training was significantly easier. Using the code from the original file, I built the P-Net. Then, I read in the positive and negative images, as well as the set of bounding box coordinates, each as an array. I gave each of the negative images bounding box coordinates of [0,0,0,0].</p><p>Then, I shuffled up the images with an index: since I loaded positive images first, all the positive images were in the beginning of the array. If I didn’t shuffle it up, the first few batches of training data would all be positive images.</p><p>Finally, I defined a cross-entropy loss function: the square of the error of each bounding box coordinate and probability.</p><iframe class="ql-video" frameborder="0" allowfullscreen="true" src="https://towardsdatascience.com/media/7eebe600353a2e8f83363cf19ce23166?postId=a60330f15fd5" height="250" width="700"></iframe><p><br></p><p>I ran the training loop. After about 30 epochs, I achieved an accuracy of around 80%…which wasn’t bad considering I only have 10000 images in my dataset.</p><p>After saving my weights, I loaded them back into the full MTCNN file, and ran a test with my newly trained P-Net. Just like before, it could still accurately identify faces and draw bounding boxes around them. A huge advantage of the MTCNN model is that even if the P-Net accuracy went down, R-Net and O-Net could still manage to refine the bounding box edges.</p><hr class="ql-align-center"><p><br></p><p>Rather than go through the tedious process of processing data for RNet and ONet again, I found&nbsp;<a href="https://github.com/wangbm/MTCNN-Tensorflow" target="_blank" style="color: inherit; background-color: transparent;">this MTCNN model</a>&nbsp;on Github which included training files for the model. This model similarly only trained bounding box coordinates (and not the facial landmarks) with the WIDER-FACE dataset.</p><p><img src="https://cdn-images-1.medium.com/max/800/1*gRdr9txrn08b_Y_79l0kSA.png"></p><p>Image 5: Intersection over&nbsp;Union</p><p>Just like I did, this model cropped each image (into 12x12 pixels for P-Net, 24x24 pixels for R-Net, and 48x48 pixels for O-Net) before the training process. Unlike my simple algorithm, this team classified images as positive or negative based on&nbsp;<strong>IoU</strong>&nbsp;(Intersection over Union, i.e. intersecting area between 12x12 image and bounding box divided by the total area of the 12x12 image and the bounding box), and included a separate category for “part” faces.</p><p>Creating a separate “part face” category allows the network to learn partially covered faces. This way, even if you wear sunglasses, or have half your face turned away, the network can still recognize your face.</p><p>In addition, for R-Net and O-Net training, they utilized&nbsp;<strong>hard sample mining</strong>. Even after training, P-Net is not perfect; it would still recognize some images with no faces in it as positive (with face) images. These images are known as&nbsp;<strong>false positives</strong>. Since R-Net’s job is to refine bounding box edges and reduce false positives, after training P-Net, we can take P-Net’s false positives and include them in R-Net’s training data. This can help R-Net target P-Net’s weaknesses and improve accuracy. This process is known as&nbsp;<strong>hard sample mining</strong>. Similarly, they applied hard sample mining in O-Net training as well.</p><p>Another interesting aspect of this model is their loss function. Instead of defining 1 loss function for both face detection and bounding box coordinates, they defined a loss function each. During the training process, they then switched back and forth between the two loss functions with every back-propagation step. I’ve never seen loss functions defined like this before — I’ve always thought it would be simpler to define one all-encompassing loss function. I wonder if switching back and forth like this improves training accuracy?</p><p>Training this model took 3 days. The large dataset made training and generating hard samples a slow process. In addition, the GPU ran out of memory the first time I trained it, forcing me to re-train R-Net and O-Net (which took another day).</p><p>I had not looked into this before, but allocating GPU memory is another vital part of the training process. At the end of each training program, they noted how much GPU memory they wanted to use and whether or not they would allow for growth. If yes, the program can ask for more memory if needed. If not, the program will allocate memory at the beginning of the program, and will not use more memory than specified throughout the whole training process. This makes the process slower, but lowers the risk of GPU running out of memory.</p><hr class="ql-align-center"><p><br></p><ul><li>Click&nbsp;<a href="https://medium.com/@reina.wang/mtcnn-face-detection-cdcb20448ce0" target="_blank" style="color: inherit; background-color: transparent;">here</a>&nbsp;to read about implementing the MTCNN model!</li><li>Click&nbsp;<a href="https://medium.com/@reina.wang/face-detection-neural-network-structure-257b8f6f85d1" target="_blank" style="color: inherit; background-color: transparent;">here</a>&nbsp;to read about the network structure of the MTCNN model!</li><li>Click&nbsp;<a href="https://towardsdatascience.com/how-does-a-face-detection-program-work-using-neural-networks-17896df8e6ff" target="_blank" style="color: inherit; background-color: transparent;">here</a>&nbsp;to read about how the MTCNN model works!</li></ul><hr class="ql-align-center"><p><br></p><p>Download the MTCNN paper and resources here:</p><ul><li>Trained model Github download:<a href="https://github.com/ipazc/mtcnn" target="_blank" style="color: inherit; background-color: transparent;">&nbsp;https://github.com/ipazc/mtcnn</a></li><li>Trainable model Github download:&nbsp;<a href="https://github.com/wangbm/MTCNN-Tensorflow" target="_blank" style="color: inherit; background-color: transparent;">https://github.com/wangbm/MTCNN-Tensorflow</a></li><li>Research article:&nbsp;<a href="http://arxiv.org/abs/1604.02878" target="_blank" style="color: inherit; background-color: transparent;">http://arxiv.org/abs/1604.02878</a></li><li>My PNet training code:&nbsp;<a href="https://github.com/reinaw1012/pnet-training" target="_blank" style="color: inherit; background-color: transparent;">https://github.com/reinaw1012/pnet-training</a></li></ul><p><br></p>